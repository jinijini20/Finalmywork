# vector_db.py
import json, uuid, glob, os
import chromadb
from sentence_transformers import SentenceTransformer
from chromadb.utils.embedding_functions import EmbeddingFunction

DB_PATH = "./chroma_db"
COLLECTION_NAME = "qna_collection"

# 1) ì•„ëž˜ ë‘˜ ì¤‘ í•˜ë‚˜ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
# (A) ê¸€ë¡­ íŒ¨í„´: í•˜ìœ„ í´ë” í¬í•¨ ëª¨ë“  jsonl
# (B) ëª…ì‹œì  ë¦¬ìŠ¤íŠ¸: íŠ¹ì • íŒŒì¼ë§Œ
JSONL_FILES = []  # ["generated_qa_people.jsonl", "more_qa.jsonl"]

TOP_K = 3
BATCH_SIZE = 1000  # ë©”ëª¨ë¦¬/ì„±ëŠ¥ ê· í˜•ìš© ë°°ì¹˜ ì—…ì„œíŠ¸

class BGEPassageEmbedding(EmbeddingFunction):
    def __init__(self, model_name="BAAI/bge-m3", normalize=True, device=None):
        self.model = SentenceTransformer(model_name, device=device)
        self.normalize = normalize
    def __call__(self, texts):
        texts = [f"passage: {t}" for t in texts]
        embs = self.model.encode(texts, normalize_embeddings=self.normalize)
        return embs.tolist()

bge_model = SentenceTransformer("BAAI/bge-m3")
NORMALIZE = True

client = chromadb.PersistentClient(path=DB_PATH)
collection = client.get_or_create_collection(
    name=COLLECTION_NAME,
    embedding_function=BGEPassageEmbedding("BAAI/bge-m3", normalize=NORMALIZE),
    metadata={"hnsw:space": "cosine"}
)

# ðŸ‘‡ ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìžì—´ë¡œ ì•ˆì „ ë³€í™˜
def to_meta_value(v):
    if isinstance(v, (str, int, float, bool)) or v is None:
        return v
    try:
        return json.dumps(v, ensure_ascii=False)
    except Exception:
        return str(v)

def iter_jsonl_files():
    files = []
    if JSONL_FILES:
        files.extend(JSONL_FILES)
    if JSONL_GLOB:
        files.extend(glob.glob(JSONL_GLOB, recursive=True))
    # ì¤‘ë³µ ì œê±° + ì¡´ìž¬í•˜ëŠ” íŒŒì¼ë§Œ
    seen = set()
    for p in files:
        p = os.path.abspath(p)
        if p not in seen and os.path.isfile(p):
            seen.add(p)
            yield p

def upsert_batch(docs, metadatas, ids):
    if not docs:
        return 0
    collection.upsert(documents=docs, metadatas=metadatas, ids=ids)
    return len(ids)

def ingest_files():
    total = 0
    docs, ids, metadatas = [], [], []

    for filepath in iter_jsonl_files():
        print(f"\nðŸ“„ íŒŒì¼ ì²˜ë¦¬ ì‹œìž‘: {filepath}")
        with open(filepath, "r", encoding="utf-8") as f:
            for ln, line in enumerate(f, start=1):
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError as e:
                    print(f"  âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (line {ln}): {e}")
                    continue

                q = obj.get("question", "")
                a = obj.get("answer", "")
                _id = obj.get("id") or str(uuid.uuid4())

                raw_meta = {
                    "sources": obj.get("sources"),
                    "tags": obj.get("tags"),
                    "last_verified": obj.get("last_verified"),
                    "source_file": obj.get("source_file") or os.path.basename(filepath),
                }
                meta = {k: to_meta_value(v) for k, v in raw_meta.items()}

                docs.append(f"Q: {q}\nA: {a}")
                ids.append(_id)
                metadatas.append(meta)

                # ë°°ì¹˜ ì—…ì„œíŠ¸
                if len(docs) >= BATCH_SIZE:
                    n = upsert_batch(docs, metadatas, ids)
                    total += n
                    print(f"  â†³ ë°°ì¹˜ ì—…ì„œíŠ¸ ì™„ë£Œ: +{n} (ëˆ„ì  {total})")
                    docs, ids, metadatas = [], [], []

    # ìž”ì—¬ë¶„ ì—…ì„œíŠ¸
    n = upsert_batch(docs, metadatas, ids)
    total += n
    if n:
        print(f"  â†³ ë§ˆì§€ë§‰ ë°°ì¹˜ ì—…ì„œíŠ¸: +{n} (ëˆ„ì  {total})")

    print(f"\nâœ… ì „ì²´ ì—…ì„œíŠ¸ ì™„ë£Œ: {total}ê°œ")
    return total

if __name__ == "__main__":
    # ë°ì´í„° ì ìž¬
    ingest_files()

    # ì¡°íšŒ ì˜ˆì‹œ (ìœ ì‚¬ë„ %ë¡œ í‘œì‹œ)
    user_query = "CardDAV APIì—ì„œ ì—°ë½ì²˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì€?"
    query_emb = bge_model.encode([f"query: {user_query}"], normalize_embeddings=NORMALIZE).tolist()
    results = collection.query(query_embeddings=query_emb, n_results=TOP_K)

    for i, (doc, meta, _id, dist) in enumerate(zip(
        results["documents"][0],
        results["metadatas"][0],
        results["ids"][0],
        results["distances"][0],
    ), start=1):
        similarity = max(0.0, min(1.0, 1 - dist))
        sim_pct = round(similarity * 100, 1)
        print(f"\n[{i}] id={_id}  similarity={sim_pct}%  (distance={dist:.4f})")
        print(doc)
        print("-> sources:", meta.get("sources"))
        print("-> tags:", meta.get("tags"))
        print("-> last_verified:", meta.get("last_verified"))
        print("-> source_file:", meta.get("source_file"))
